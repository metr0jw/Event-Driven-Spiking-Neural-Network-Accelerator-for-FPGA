# Event-Driven Spiking Neural Network Accelerator for FPGA

**Energy-efficient Event-driven Spiking Neural Network accelerator for FPGA with PyTorch integration**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Development Environment
- **FPGA Tools**: Vivado 2025.1, Vitis HLS 2025.1  
- **Target Hardware**: PYNQ-Z2 (Xilinx Zynq-7000, xc7z020clg400-1)
- **Software**: Python 3.13+, PyTorch 2.9.0+, PYNQ 2.7+
- **HDL Standard**: Verilog-2001 (IEEE 1364-2001)

## Description

This project implements a complete event-driven spiking neural network (SNN) accelerator on FPGA that integrates seamlessly with PyTorch for training and deployment. Unlike traditional clock-driven accelerators, this design uses event-driven processing that is more familiar to software engineers and more efficient for sparse neural computations.

### Key Features

ğŸ§  **Biologically-Inspired Neurons**
- Leaky Integrate-and-Fire (LIF) neuron model
- Support for both excitatory and inhibitory neurons
- Configurable membrane dynamics and refractory periods
- Hardware-optimized fixed-point arithmetic

âš¡ **Energy-Efficient AC-Based Architecture**
- **Accumulate-only (AC) operations** instead of Multiply-Accumulate (MAC)
- ~5x energy reduction per synaptic operation (0.9pJ vs 4.6pJ at 45nm)
- Exploits binary spike nature: spike Ã— weight = weight (when spike=1)
- Sparse processing: 90-99% operations skipped when spike=0
- Real-time energy monitoring module

ğŸ“š **Advanced Learning Algorithms**
- Standard STDP (Spike-Timing Dependent Plasticity)
- R-STDP (Reward-modulated STDP) for reinforcement learning
- Triplet STDP for enhanced stability (TODO)
- Adaptive learning rate mechanisms (TODO)

ğŸ”„ **Event-Driven Architecture**
- Asynchronous spike processing
- AXI-Stream interfaces for efficient data flow
- Time-multiplexed neuron arrays for scalability
- Optimized for sparse neural activity

ï¿½ï¿½ **PyTorch Integration**
- Seamless model conversion from PyTorch to SNN
- Multiple spike encoding schemes (Poisson, temporal, rate-based)
- Hardware-in-the-loop training support
- Gradient-free STDP training loops for quick experimentation
- Comprehensive visualization tools

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PyTorch       â”‚    â”‚   FPGA SNN       â”‚    â”‚   Output        â”‚
â”‚   Training      â”‚â”€â”€â”€â–¶â”‚   Accelerator    â”‚â”€â”€â”€â–¶â”‚   Processing    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ SNN Models  â”‚ â”‚    â”‚ â”‚ LIF Neurons  â”‚ â”‚    â”‚ â”‚ Spike       â”‚ â”‚
â”‚ â”‚ STDP/R-STDP â”‚ â”‚    â”‚ â”‚ AC Synapses  â”‚ â”‚    â”‚ â”‚ Decoding    â”‚ â”‚
â”‚ â”‚ Encoding    â”‚ â”‚    â”‚ â”‚ Conv/FC/Pool â”‚ â”‚    â”‚ â”‚ Analysis    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Energy Efficiency: AC vs MAC

This accelerator uses **Accumulate-only (AC)** operations instead of traditional **Multiply-Accumulate (MAC)** operations, significantly reducing energy consumption.

### Why AC Works for SNNs

In SNNs, spikes are binary (0 or 1):
- When spike = 0: No computation needed (sparse optimization)
- When spike = 1: `spike Ã— weight = weight` (no multiplication required)

### Energy Comparison (45nm Technology)

| Operation | Energy | Relative |
|-----------|--------|----------|
| MAC (32-bit FP) | ~4.6 pJ | 1.0x |
| AC (16-bit INT) | ~0.9 pJ | 0.2x |

### Total Energy Savings

```
ANN Energy:  N_ops Ã— 4.6pJ (MAC)
SNN Energy:  N_ops Ã— sparsity Ã— 0.9pJ (AC)

With 5% spike activity: ~100x energy reduction
With 10% spike activity: ~50x energy reduction
```

## Implementation Status

### Completed Features
- [x] **LIF Neuron Core**: Hardware implementation with excitatory/inhibitory support
- [x] **AC-Based Neurons**: Energy-efficient accumulate-only neuron arrays
- [x] **AC-Based Synapses**: Sparse synapse arrays with sparsity exploitation
- [x] **Layer Support**: Conv1D, Conv2D, FC, MaxPool, AvgPool (all AC-based)
- [x] **Energy Monitor**: Real-time energy estimation module
- [x] **STDP Learning**: Standard spike-timing dependent plasticity
- [x] **R-STDP Learning**: Reward-modulated plasticity for RL
- [x] **PyTorch Interface**: Model conversion and weight loading
- [x] **Spike Encoding**: Poisson, temporal, and rate-based encoders
- [x] **Comprehensive Testbenches**: Event-driven verification
- [x] **Python Software Stack**: Complete API and utilities
- [x] **Training Examples**: MNIST classification and RL navigation
- [x] **Communication Interface**: AXI-based PC communication

### In Progress
- [ ] **Fix bugs in existing features**
- [ ] **Advanced Connectivity**: Recurrent layer support
- [ ] **Optimized Bitstreams**: Power and performance optimization
- [ ] **Real-time Demos**: Live inference applications

## Quick Start

### Installation
```bash
# Clone the repository
git clone https://github.com/metr0jw/Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA.git
cd Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA

# Activate environment (if using provided virtualenv)
source venv/bin/activate

# Run automated setup
./setup.sh
```

### Basic Usage
```bash
# Quick demonstration
python quick_start.py

# PyTorch training example (STDP-based)
python examples/pytorch/mnist_training_example.py

# R-STDP learning example
python examples/pytorch/r_stdp_learning_example.py
```

For detailed usage instructions, see the [User Guide](docs/user_guide.md).

## Usage Examples

See the [User Guide](docs/user_guide.md) for comprehensive usage examples and tutorials.

### Quick Example
```python
from snn_fpga_accelerator import SNNAccelerator

# Initialize and configure
accelerator = SNNAccelerator(bitstream_path="hardware/bitstream.bit")
accelerator.configure_network(network_config)

# Run inference
output = accelerator.infer(input_spikes)
```

More examples available in the `examples/` directory.

## Project Structure

```
â”œâ”€â”€ hardware/                    # FPGA implementation
â”‚   â”œâ”€â”€ hdl/rtl/                # Verilog RTL sources
â”‚   â”‚   â”œâ”€â”€ neurons/            # LIF neuron implementations (AC-based)
â”‚   â”‚   â”œâ”€â”€ synapses/           # Synaptic weight memory (AC-based)
â”‚   â”‚   â”œâ”€â”€ layers/             # Conv1D/2D, FC, Pooling layers
â”‚   â”‚   â”œâ”€â”€ router/             # Spike routing logic
â”‚   â”‚   â”œâ”€â”€ common/             # Utilities, Energy monitor
â”‚   â”‚   â””â”€â”€ top/                # Top-level integration
â”‚   â”œâ”€â”€ hdl/tb/                 # Comprehensive testbenches
â”‚   â”œâ”€â”€ hls/                    # High-Level Synthesis
â”‚   â”‚   â”œâ”€â”€ src/                # Learning algorithms
â”‚   â”‚   â”œâ”€â”€ include/            # Header files
â”‚   â”‚   â””â”€â”€ test/               # HLS testbenches
â”‚   â””â”€â”€ scripts/                # Build automation
â”œâ”€â”€ software/python/            # Python software stack
â”‚   â””â”€â”€ snn_fpga_accelerator/   # Main package
â”‚       â”œâ”€â”€ accelerator.py      # FPGA interface
â”‚       â”œâ”€â”€ pytorch_interface.py # PyTorch integration
â”‚       â”œâ”€â”€ spike_encoding.py   # Encoding algorithms
â”‚       â”œâ”€â”€ learning.py         # STDP/R-STDP
â”‚       â””â”€â”€ utils.py            # Utilities
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â”œâ”€â”€ pytorch/                # PyTorch examples
â”‚   â””â”€â”€ notebooks/              # Jupyter notebooks
â””â”€â”€ docs/                       # Documentation
```

## Key Components

For detailed architecture information, see the [Architecture Documentation](docs/architecture.md).

### LIF Neuron Model (AC-Based)
- Membrane integration with shift-based leak (no multiply)
- Accumulate-only synaptic integration
- Configurable refractory periods
- Saturation arithmetic for fixed-point implementation

### AC-Based Synapse Array
- Sparse processing: skips spike=0 inputs entirely
- Weight accumulation without multiplication
- Built-in energy monitoring counters

### Layer Implementations
- **snn_conv1d_ac.v**: 1D convolution for temporal data
- **snn_conv2d_ac.v**: 2D convolution for image processing
- **snn_fc_ac.v**: Fully connected layer for classification
- **snn_maxpool1d/2d.v**: Max pooling layers
- **snn_avgpool1d/2d.v**: Average pooling layers

### STDP Learning Rule
- Long-term potentiation (LTP) and depression (LTD)
- Eligibility traces for reward-modulated learning
- Configurable time constants and learning rates

### Energy Monitor
- Real-time AC operation counting
- Memory access tracking
- ANN vs SNN energy comparison

## Performance Characteristics

| Metric | Current Implementation | Planned/Projected | Notes |
|--------|------------------------|-------------------|-------|
| **Max Neurons** | 64 neurons synthesized and verified | â‰¥1024 neurons via time-multiplexing | Scaling gated by BRAM budget |
| **Spike Throughput** | Characterization in progress | â‰¥100K spikes/s at 100 MHz | One spike per cycle after optimization |
| **End-to-End Latency** | Microsecond-scale pipeline | <10 Âµs per spike | Latency dominated by AXI stages |
| **Power (Board)** | TBD (hardware bring-up scheduled) | ~2 W on PYNQ-Z2 | Estimate from Vivado power analysis |
| **Energy Efficiency** | ~5x vs MAC-based | ~100x with sparsity | AC operations + sparse processing |
| **Numeric Precision** | 8-bit weights, 16-bit membrane | Higher precision paths available | Fixed-point format |

### ğŸ“š Documentation
- **User Guide**: Updated with CNN conversion examples and step mode usage
- **IMPROVEMENTS.md**: Detailed summary of architectural decisions
- **AXI_INTERFACE_BEST_PRACTICES.md**: Guide for safer AXI interface development
- **COCOTB_INTEGRATION_GUIDE.md**: RTL verification strategy with Python testbenches

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow
1. **Fork** the repository
2. **Create** a feature branch
3. **Implement** your changes with tests
4. **Submit** a pull request

## Documentation

- ğŸ“– **[User Guide](docs/user_guide.md)**: Comprehensive usage instructions and tutorials
- ğŸ—ï¸ **[Architecture](docs/architecture.md)**: System design and component details
- ğŸ“š **[API Reference](docs/api_reference.md)**: Complete API documentation
- ğŸ”§ **[Developer Guide](docs/developer_guide.md)**: Development setup and guidelines
- ğŸ¤ **[Contributing](CONTRIBUTING.md)**: Contribution guidelines

## Citation

If you use this work in your research, please cite:

```bibtex
@misc{snn_fpga_accelerator,
  title={Event-Driven Spiking Neural Network Accelerator for FPGA},
  author={Jiwoon Lee},
  year={2025},
  howpublished={\url{https://github.com/metr0jw/Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA}}
}
```

## References

- Energy efficiency analysis based on: Horowitz, M. "Computing's Energy Problem (and what we can do about it)" ISSCC 2014
- SNN sparsity benefits: Roy, K. et al. "Towards spike-based machine intelligence with neuromorphic computing" Nature 2019

## Troubleshooting

For common issues and solutions, see the [User Guide - Troubleshooting](docs/user_guide.md#troubleshooting) and [Developer Guide - Common Issues](docs/developer_guide.md#troubleshooting).

## Support

- **Issues**: [GitHub Issues](https://github.com/metr0jw/Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA/issues)
- **Email**: [jwlee@linux.com](mailto:jwlee@linux.com)
- **Documentation**: [Project Wiki](https://github.com/metr0jw/Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA/wiki)

## License
[MIT License](LICENSE)

## Author
[Jiwoon Lee](https://github.com/metr0jw)
